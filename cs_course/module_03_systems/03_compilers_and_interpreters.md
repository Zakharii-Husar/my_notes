# Compilers and Interpreters

Every line of code you write goes through a transformation pipeline before it runs — whether compiled ahead of time (C, Rust, Go), interpreted (Python, Ruby), or JIT-compiled at runtime (Java, JavaScript, C#). Understanding this pipeline demystifies error messages, explains why some languages are faster than others, and is essential if you ever build a DSL, query engine, template system, or configuration language. Even if you never write a compiler, you'll read and debug their output.

---

## 1. The Compilation Pipeline

Source code is transformed through a series of stages. Each stage produces a well-defined intermediate representation consumed by the next.

```
Source Code
    │
    ▼
┌──────────┐
│  Lexer   │  → Tokens       (characters → words)
└──────────┘
    │
    ▼
┌──────────┐
│  Parser  │  → AST          (words → sentence structure)
└──────────┘
    │
    ▼
┌────────────────────┐
│ Semantic Analysis  │  → Annotated AST  (meaning & type checks)
└────────────────────┘
    │
    ▼
┌──────────┐
│ IR Gen   │  → Intermediate Representation (machine-independent)
└──────────┘
    │
    ▼
┌──────────────┐
│ Optimization │  → Optimized IR  (faster, smaller)
└──────────────┘
    │
    ▼
┌──────────┐
│ Codegen  │  → Machine code / bytecode
└──────────┘
```

Not every language uses every stage. Interpreters may execute directly from the AST. JIT compilers defer codegen and optimization to runtime. But the conceptual stages remain the same.

---

## 2. Lexical Analysis (Lexer / Tokenizer)

The lexer reads raw source text character by character and groups them into **tokens** — the "words" of the language. It strips whitespace and comments, identifies keywords, operators, literals, and identifiers.

### How It Works

A lexer is typically implemented as a finite automaton (state machine) or with regular expressions:

```
Input:  "if (x >= 42) return true;"

Output tokens:
  IF        "if"
  LPAREN    "("
  IDENT     "x"
  GTE       ">="
  INT_LIT   "42"
  RPAREN    ")"
  RETURN    "return"
  TRUE      "true"
  SEMICOLON ";"
```

### Implementation Example

```python
import re

TOKEN_SPEC = [
    ("NUMBER",   r"\d+"),
    ("IDENT",    r"[a-zA-Z_]\w*"),
    ("OP",       r"[+\-*/=<>!]=?"),
    ("LPAREN",   r"\("),
    ("RPAREN",   r"\)"),
    ("SKIP",     r"[ \t]+"),
    ("NEWLINE",  r"\n"),
]

TOKEN_RE = re.compile("|".join(f"(?P<{name}>{pattern})" for name, pattern in TOKEN_SPEC))

def tokenize(source):
    tokens = []
    for match in TOKEN_RE.finditer(source):
        kind = match.lastgroup
        value = match.group()
        if kind == "SKIP" or kind == "NEWLINE":
            continue
        if kind == "IDENT" and value in ("if", "else", "return", "true", "false"):
            kind = value.upper()  # promote to keyword token
        tokens.append((kind, value))
    return tokens

print(tokenize("if x >= 42 return true"))
# [('IF', 'if'), ('IDENT', 'x'), ('OP', '>='), ('NUMBER', '42'),
#  ('RETURN', 'return'), ('TRUE', 'true')]
```

### Key Points

- Lexers report the first class of errors: **lexical errors** (e.g., unterminated string literal `"hello`, invalid character `@` in a language that doesn't use it).
- Lexers are fast — they run in O(n) over the source text.
- Tools like Flex (C) or ANTLR can generate lexers from grammar specifications.

---

## 3. Parsing

The parser takes a stream of tokens and builds an **Abstract Syntax Tree (AST)** — a hierarchical structure that reflects the grammatical structure of the program.

### Grammars

Languages are defined by **context-free grammars** (CFGs), typically written in Backus-Naur Form (BNF) or its extended variant (EBNF):

```
expression  = term (('+' | '-') term)*
term        = factor (('*' | '/') factor)*
factor      = NUMBER | IDENT | '(' expression ')'
```

This grammar naturally encodes **operator precedence**: multiplication binds tighter than addition because `term` is nested inside `expression`.

### Parsing Strategies

- **Recursive descent** (top-down): Each grammar rule becomes a function. Simple, readable, and the most common approach for hand-written parsers. Used by GCC (for C), Go's compiler, and many others.

- **Pratt parsing** (top-down operator precedence): Elegant for expressions with many precedence levels. Each operator has a binding power.

- **LR / LALR parsers** (bottom-up): Generated by tools like Yacc/Bison. More powerful but harder to debug. Used historically by C compilers.

### Recursive Descent Example

```python
class Parser:
    def __init__(self, tokens):
        self.tokens = tokens
        self.pos = 0

    def peek(self):
        return self.tokens[self.pos] if self.pos < len(self.tokens) else None

    def consume(self, expected_kind=None):
        token = self.tokens[self.pos]
        if expected_kind and token[0] != expected_kind:
            raise SyntaxError(f"Expected {expected_kind}, got {token}")
        self.pos += 1
        return token

    def parse_expression(self):
        """expression = term (('+' | '-') term)*"""
        left = self.parse_term()
        while self.peek() and self.peek()[1] in ("+", "-"):
            op = self.consume()
            right = self.parse_term()
            left = ("binop", op[1], left, right)
        return left

    def parse_term(self):
        """term = factor (('*' | '/') factor)*"""
        left = self.parse_factor()
        while self.peek() and self.peek()[1] in ("*", "/"):
            op = self.consume()
            right = self.parse_factor()
            left = ("binop", op[1], left, right)
        return left

    def parse_factor(self):
        """factor = NUMBER | '(' expression ')'"""
        token = self.peek()
        if token[0] == "NUMBER":
            self.consume()
            return ("number", int(token[1]))
        elif token[1] == "(":
            self.consume()
            expr = self.parse_expression()
            self.consume()  # ')'
            return expr
        raise SyntaxError(f"Unexpected token: {token}")
```

For input `3 + 4 * 2`, this produces:

```
("binop", "+",
    ("number", 3),
    ("binop", "*",
        ("number", 4),
        ("number", 2)))
```

Notice how `*` is deeper in the tree — it will be evaluated first, correctly implementing precedence.

---

## 4. Semantic Analysis

After parsing, the compiler checks that the program **means something valid**. Parsing ensures syntax is correct; semantic analysis ensures the logic is sound.

### 4.1 Symbol Tables

A **symbol table** maps identifiers to their declarations (type, scope, memory location). As the compiler walks the AST:

1. When entering a new scope (function, block), push a new scope onto the symbol table stack.
2. When declaring a variable, add it to the current scope. Error if it already exists (redeclaration).
3. When referencing a variable, search from the innermost scope outward. Error if not found (undefined variable).

```
function foo(x: int) {        // scope 1: {x: int}
    let y = x + 1;            // scope 1: {x: int, y: int}
    if (y > 0) {
        let z = y * 2;        // scope 2: {z: int} (parent: scope 1)
        print(x + z);         // resolves x from scope 1, z from scope 2
    }
    // z is not accessible here — scope 2 was popped
}
```

### 4.2 Type Checking

The type checker walks the annotated AST and verifies:
- Operands have compatible types (`3 + "hello"` → type error)
- Functions are called with the correct number and types of arguments
- Return types match declarations
- Assignments match declared types

**Static type checking** (compile time) catches errors early but requires type annotations or inference. **Dynamic type checking** (runtime) is more flexible but errors only appear when the code runs.

**Type inference** lets the compiler deduce types without explicit annotations:

```rust
let x = 5;          // inferred as i32
let y = x + 3.0;    // error: can't add i32 and f64
let names = vec![];  // error: can't infer element type (need context)
```

### 4.3 Scopes

Scoping rules determine which declarations are visible where:

- **Lexical (static) scoping**: A variable's scope is determined by its position in the source code. Used by most modern languages. A function sees variables from the scope where it was *defined*, not where it's *called*.
- **Dynamic scoping**: A variable's scope is determined by the call stack at runtime. Rare today (some shell languages, early Lisps). Fragile and hard to reason about.

```javascript
// Lexical scoping — JavaScript closures
function makeCounter() {
    let count = 0;
    return function() {
        count++;          // 'count' resolved from enclosing scope
        return count;     // even after makeCounter() has returned
    };
}
const counter = makeCounter();
counter(); // 1
counter(); // 2
```

---

## 5. Intermediate Representation and Optimization

### 5.1 Intermediate Representation (IR)

After semantic analysis, the compiler lowers the AST into an **intermediate representation** — a simpler, more uniform language that's easier to optimize and closer to machine code but still machine-independent.

Common IR forms:
- **Three-address code**: Each instruction has at most three operands: `t1 = a + b`
- **SSA (Static Single Assignment)**: Each variable is assigned exactly once. Makes data flow analysis trivial. Used by LLVM, GCC, V8.
- **Stack-based bytecode**: Operations push/pop from a virtual stack. Used by JVM, CPython, WebAssembly.

```
Source:      x = (a + b) * (c - d)

Three-address code:
  t1 = a + b
  t2 = c - d
  x  = t1 * t2

SSA form:
  t1 = add a, b
  t2 = sub c, d
  x1 = mul t1, t2
```

### 5.2 Common Optimizations

| Optimization | What It Does | Example |
|---|---|---|
| Constant folding | Evaluate constant expressions at compile time | `3 + 4` → `7` |
| Dead code elimination | Remove code that has no effect | `if (false) { ... }` → removed |
| Common subexpression elimination | Reuse previously computed values | `a*b + a*b` → `t=a*b; t+t` |
| Inlining | Replace function call with function body | Eliminates call overhead |
| Loop-invariant code motion | Move computations that don't change out of loops | `for(i) x = a+b; use(x)` → `x=a+b; for(i) use(x)` |
| Strength reduction | Replace expensive operations with cheaper ones | `x * 2` → `x << 1` |
| Register allocation | Map variables to CPU registers to avoid memory access | Graph coloring algorithm |

**The optimizer's golden rule**: Transformations must not change the program's observable behavior.

---

## 6. Code Generation

The final stage translates optimized IR into target code:
- **Native machine code** (x86, ARM): Compiled languages — C, C++, Rust, Go.
- **Bytecode** for a virtual machine: JVM (Java/Kotlin), CLR (.NET), CPython.
- **Another source language**: Transpilers — TypeScript → JavaScript, Kotlin → JVM bytecode.

Key challenges in codegen:
- **Register allocation**: CPUs have limited registers. The compiler must decide which values live in registers vs memory (spilling).
- **Instruction selection**: Mapping IR operations to specific machine instructions (some CPUs have specialized instructions for common patterns).
- **Instruction scheduling**: Reorder instructions to maximize pipeline utilization and avoid stalls.

---

## 7. Garbage Collection

Languages that manage memory automatically use a **garbage collector** (GC) to reclaim objects that are no longer reachable from the program's roots (global variables, stack frames, registers).

### 7.1 Mark and Sweep

The simplest GC algorithm:

1. **Mark phase**: Starting from roots, traverse all reachable objects and mark them.
2. **Sweep phase**: Scan the heap; any unmarked object is garbage — reclaim its memory.

```
Before GC:
  Roots → [A] → [B] → [C]
                  ↗
           [D] ──┘
           [E]          ← unreachable
           [F] → [G]   ← unreachable

After mark phase:
  [A]✓ [B]✓ [C]✓ [D]✓ [E]✗ [F]✗ [G]✗

After sweep:
  [A] [B] [C] [D] — E, F, G reclaimed
```

**Pros**: Handles cycles (unlike reference counting).
**Cons**: Must pause the program during collection ("stop-the-world").

### 7.2 Generational GC

Based on the **generational hypothesis**: most objects die young.

The heap is divided into generations:
- **Young generation (nursery)**: Newly allocated objects. Collected frequently (minor GC). Most objects are garbage — collection is fast.
- **Old generation (tenured)**: Objects that survived several young GC cycles. Collected rarely (major GC). More expensive but less frequent.

```
Allocation → Young Gen ──(survives N collections)──→ Old Gen
                  │                                      │
             minor GC (fast, frequent)              major GC (slow, rare)
```

Objects in the young generation that are still alive get **promoted** to the old generation. A **write barrier** tracks references from old → young objects (since we only scan the young generation during minor GC).

**Used by**: JVM (G1, ZGC, Shenandoah), .NET, V8 (JavaScript), Go (concurrent tri-color).

### 7.3 Stop-the-World vs Incremental / Concurrent

- **Stop-the-world (STW)**: The entire application pauses while the GC runs. Simple but causes latency spikes. Acceptable for batch jobs, unacceptable for real-time systems.
- **Incremental**: GC work is broken into small steps interleaved with application execution. Shorter pauses but more total overhead.
- **Concurrent**: GC runs on separate threads simultaneously with the application. Complex (must handle objects changing during collection) but minimizes pauses.

Modern collectors like **ZGC** (Java) and **Go's GC** achieve sub-millisecond pauses even with terabytes of heap, using concurrent marking and compaction.

### 7.4 Reference Counting

An alternative to tracing GC. Each object has a count of references pointing to it. When the count drops to zero, the object is immediately freed.

**Pros**: Deterministic — objects are freed as soon as they become unreachable. No pause.
**Cons**: Cannot collect cycles (A→B→A, both count=1, neither freed). Overhead of updating counts on every assignment.

**Used by**: Python (with cycle detector for backup), Swift (ARC — compile-time inserted retain/release), Rust's `Rc<T>` / `Arc<T>`.

```python
# Python reference counting + cycle collector
import sys

a = [1, 2, 3]
print(sys.getrefcount(a))  # 2 (variable 'a' + getrefcount's arg)

b = a
print(sys.getrefcount(a))  # 3

del b
print(sys.getrefcount(a))  # 2
```

---

## 8. JIT Compilation

A **Just-In-Time (JIT) compiler** compiles code at runtime, after observing how the program actually behaves. It combines the flexibility of interpretation with the performance of native compilation.

### 8.1 How It Works

1. **Interpret first**: Run bytecode in an interpreter. This is fast to start.
2. **Profile**: Track which functions/loops are executed frequently (**hot paths**).
3. **Compile hot paths**: When a function exceeds a threshold (e.g., called 10,000 times), compile it to optimized native code.
4. **Deoptimize if needed**: If assumptions made during compilation are violated, fall back to the interpreter.

```
Execution flow:
  Cold code → Interpreter (slow but starts immediately)
       │
       ├── function called 10,000 times → JIT compiles to native code
       │                                         │
       │                                    optimized execution
       │                                         │
       └── assumption violated ← ─ ─ ─ deoptimize back to interpreter
```

### 8.2 Key JIT Optimizations

**Inline caching**: In dynamic languages, the type of `obj.method()` isn't known at compile time. Inline caching records the type seen at each call site and generates optimized code for that specific type.

```javascript
// JavaScript example — V8's inline caching
function area(shape) {
    return shape.width * shape.height;
}

// First call with {width: 5, height: 3} — V8 records the object shape
// Subsequent calls with same shape → fast path (cached property offsets)
// Call with a different shape → fall back to slow path, re-cache
```

**Speculative optimization**: The JIT assumes the common case (e.g., a variable is always an integer) and generates code for that case. If the assumption breaks, it deoptimizes.

**On-stack replacement (OSR)**: Replace a running interpreted function with its compiled version mid-execution — important for long-running loops that should be JIT-compiled without waiting for the function to return.

### 8.3 Tiered Compilation

Modern JITs use multiple compilation tiers:

```
Tier 0:  Interpreter          — no compilation overhead
Tier 1:  Baseline compiler    — fast compilation, moderate speedup
Tier 2:  Optimizing compiler  — slow compilation, maximum speedup

Java HotSpot:  Interpreter → C1 (client compiler) → C2 (server compiler)
V8:            Ignition (interpreter) → Sparkplug (baseline) → Maglev → TurboFan (optimizing)
```

Each tier makes a tradeoff: more compilation time yields faster code, but only pays off if the code runs enough times to amortize the compilation cost.

---

## Exercises

1. **Build a tokenizer**: Write a lexer for a simple expression language supporting integers, identifiers, `+`, `-`, `*`, `/`, `(`, `)`, and `=`. Handle errors for invalid characters. Test with input like `x = (3 + y) * 2`.

2. **Recursive descent parser**: Extend your tokenizer into a parser that builds an AST for arithmetic expressions with correct operator precedence (`*`/`/` before `+`/`-`). Evaluate the AST to compute results.

3. **Scope resolution**: Implement a symbol table with nested scopes. Walk a simple AST of variable declarations and references. Report errors for undefined variables and redeclarations within the same scope.

4. **GC simulation**: Write a mark-and-sweep garbage collector for a simulated heap. Create objects with references, make some unreachable, run the collector, and verify only reachable objects survive. Then create a reference cycle and confirm it's collected.

5. **JIT observation**: In Node.js, write a function that does arithmetic and call it in a loop 1 million times. Use `--trace-opt` and `--trace-deopt` flags to see V8 optimize and (if you change argument types mid-loop) deoptimize it. Observe the performance difference.

---

## Recommended Resources

- **"Crafting Interpreters"** (Nystrom) — free online at [craftinginterpreters.com](https://craftinginterpreters.com). Build a complete interpreter in two passes: tree-walk (Java) and bytecode (C). The best hands-on resource.
- **"Compilers: Principles, Techniques, and Tools"** (Aho, Lam, Sethi, Ullman) — the "Dragon Book." Dense but comprehensive. Good as a reference.
- **"Engineering a Compiler"** (Cooper & Torczon) — more modern and readable than the Dragon Book.
- **"A Unified Theory of Garbage Collection"** (Bacon, Cheng, Rajan) — shows that tracing and reference counting are duals of each other.
- **V8 blog** ([v8.dev/blog](https://v8.dev/blog)) — practical articles on JIT compilation, inline caching, and optimization in a production engine.
- **LLVM tutorial: "My First Language Frontend"** — build a simple language targeting LLVM IR. Excellent for understanding IR and codegen.
